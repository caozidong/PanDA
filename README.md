<div align="center">
<h1>PanDA</h1>

[**Zidong Cao**](https://scholar.google.com/citations?user=q1FcZzIAAAAJ&hl=zh-CN)<sup>1</sup> · [**Jinjing Zhu**](https://scholar.google.com/citations?user=EVpo9eQAAAAJ&hl=zh-CN)<sup>1</sup> · [**Weiming Zhang**](https://scholar.google.com/citations?user=cdtgqkgAAAAJ&hl=zh-CN)<sup>1</sup> · [**Hao Ai**](https://scholar.google.com/citations?user=QNlF0DsAAAAJ&hl=zh-CN)<sup>2</sup> · [**Haotian Bai**](https://scholar.google.com/citations?user=DIy4cA0AAAAJ&hl=zh-CN)<sup>1</sup>
<br>
[**Hengshuang Zhao**](https://hszhao.github.io/)<sup>3</sup> · [**Lin Wang**](https://scholar.google.com/citations?user=SReb2csAAAAJ&hl=zh-CN)<sup>4&dagger;</sup>

<sup>1</sup>AI Thrust, HKUST (GZ)&emsp;<sup>2</sup>University of Birmingham&emsp;<sup>3</sup>HKU&emsp;<sup>4</sup>NTU
<br>
&dagger;corresponding author

<a href=""><img src='https://img.shields.io/badge/arXiv-PanDA-red' alt='Paper PDF'></a>
<a href='https://caozidong.github.io/PanDA_Depth/'><img src='https://img.shields.io/badge/Project_Page-PanDA-green' alt='Project Page'></a>
<a href='https://huggingface.co/spaces/ZidongC/PanDA_Panoramic_Depth_Estimation'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>
</div>

We propose a semi-supervised learning framework to learn a panoramic Depth Anything, dubbed PanDA. PanDA first learns a teacher model by fine-tuning Depth Anything through joint training on synthetic indoor and outdoor panoramic datasets. Then, a student model is trained using large-scale unlabeled data, leveraging pseudo-labels generated by the teacher model. PanDA exhibits impressive zero-shot capability across diverse scenes.

![teaser](assets/teaser.png)

## News
- **2025-03-18:** Code release.
- **2025-02-27:** PanDA is accepted by CVPR 2025.
- **2025-02-06:** Our survey about 360 vision is accepted by IJCV. Hope the survey helpful for you. [[Link]](https://arxiv.org/abs/2502.10444)

## Self-captured Datasets

In our manuscript, some panoramas (RGB, without depth labels) are captured by ourselves. The dataset link is [here](https://huggingface.co/datasets/Any360D/Diverse360/tree/main). It contains about 10,000 panoramas of 4K resolution. Note that we do not claim the dataset is a technical contribution.

## Pre-trained Models

We provide **three models** for robust relative panoramic depth estimation (predict depth values, range 0~1):

| Model | Params | Checkpoint |
|:-|-:|:-:|
| PanDA-Small | 24.8M | [Download](https://huggingface.co/ZidongC/PanDA/resolve/main/panda_small.pth?download=true) |
| PanDA-Base | 97.5M | [Download](https://huggingface.co/ZidongC/PanDA/resolve/main/panda_base.pth?download=true) |
| PanDA-Large | 335.3M | [Download](https://huggingface.co/ZidongC/PanDA/resolve/main/panda_large.pth?download=true) |

## Usage

### Prepraration

```bash
git clone https://github.com/caozidong/PanDA
cd PanDA
pip install -r requirements.txt
```

**Note**: We use python==3.10, and pytorch==2.0.0, cuda==11.7, and cudnn==8.5.0.

Download the checkpoints listed [here](#pre-trained-models) and put them under the `checkpoints` directory.

### Inference for *images*

```bash
python run.py \
  --config ./config/inference/panda_<large, base, small> \
  --img-path <path> --outdir <outdir> \
  [--height <height>] [--resize] [--pred-only] \
  [--grayscale] [--save-cloud]
```
Options:
- `--config`: Model config files.
- `--img-path`: It supports an image directory, a single image path, and a text file storing image paths.
- `--height`: The height of ERP image. The width is [2 x height]. By default, the height is 504. Increasing the height can obtain better predictions (1008x2016 requires more than 40GB GPU memory).
- `--resize` (optional): If resizing the output depth to have the same spatial resolution as the input ERP image.
- `--pred-only` (optional): Only save the predicted depth map, without raw image.
- `--grayscale` (optional): Save the grayscale depth map, without applying color palette.
- `--save-cloud` (optional): Save the colored point cloud result.


For example:
```bash
python run_image.py --config ./config/inference/panda_large.yaml \
       --img-path ./erp_samples/ --pred-only
```

### Inference for *videos*

```bash
python run_video.py \
  --config ./config/inference/panda_<large, base, small> \
  --video-path assets/examples_video --outdir video_depth_vis \
  [--height <height>]
```

## About teacher model training and evaluation

Please refer to [train_teacher](./train_teacher).

## About student model training and evaluation

Please refer to [train_student](./train_student).

## About Fine-tuning to Metric Depth

Please refer to [train_metric depth](./train_metric_depth).

## Acknowledgement

We sincerely thank the [Depth Anything v1](https://github.com/facebookresearch/dinov2), [Depth Anything v2](https://github.com/facebookresearch/dinov2) for contributing such impressive models and codes to our community. Also, we sincerely thank the [UniFuse](https://github.com/alibaba/UniFuse-Unidirectional-Fusion) for providing training and evaluation codes.